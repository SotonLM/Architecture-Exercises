=== Semantic Search Examples (Level 2) ===

Corpus embedding time: 0.1595 seconds

Query: How does inflation affect the economy?
Query time: 0.0171 seconds
  [21] score=0.6610 -> Inflation reduces the purchasing power of currency over time....
  [26] score=0.5156 -> Exchange rates fluctuate due to supply and demand in currency markets....
  [24] score=0.5130 -> Interest rates influence borrowing behaviour and economic growth....

Query: How can I make my pasta sauce more flavourful?
Query time: 0.0214 seconds
  [7] score=0.7596 -> Simmering tomatoes slowly can deepen the flavour of a pasta sauce....
  [8] score=0.4858 -> Fresh basil adds brightness to Mediterranean dishes....
  [10] score=0.4826 -> Stir-frying vegetables keeps their texture crisp and vibrant....

Query: What causes stars to form in the universe?
Query time: 0.0186 seconds
  [14] score=0.4278 -> A clear night sky reveals constellations scattered across the universe....
  [15] score=0.2333 -> Coastal waves erode shorelines over long periods of time....
  [20] score=0.2149 -> Deserts form in regions with extremely low annual rainfall....

Query: How do neural networks learn representations?
Query time: 0.0181 seconds
  [0] score=0.8322 -> Neural networks learn hierarchical representations from data....
  [2] score=0.4146 -> Reinforcement learning optimises behaviour by interacting with environments....
  [6] score=0.3772 -> Training deep models often requires GPUs or TPUs....

Query: Why is borrowing expensive when interest rates are high?
Query time: 0.0224 seconds
  [24] score=0.7412 -> Interest rates influence borrowing behaviour and economic growth....
  [26] score=0.4636 -> Exchange rates fluctuate due to supply and demand in currency markets....
  [27] score=0.3939 -> Fiscal policy involves government spending and taxation decisions....

=== Reflection ===

The retrieval system performs strongly for clear, well-defined factual queries. 
For example, questions about inflation, pasta sauce, neural networks, and interest 
rates all retrieved the correct top-1 and semantically related supporting paragraphs. 
However, queries with broader or more abstract semantics—such as “What causes stars 
to form?”—produce weaker matches, because the corpus does not contain a direct 
explanation of star formation, only loosely related nature descriptions. This shows 
that sentence embeddings work extremely well when the corpus contains relevant 
knowledge, but their accuracy naturally degrades when the underlying dataset lacks 
a close conceptual neighbour. Overall, the model demonstrates robust semantic 
grouping across domains, with predictable failure cases driven by corpus limitations.
