{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Importing the libraries",
   "id": "21fa8970a6f0dd34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import ViTModel\n",
    "import get_training_test_set as gts\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ],
   "id": "6a11f2d7ab9b669c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danny/PycharmProjects/face_recognition/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-21 17:34:59.215204: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-21 17:34:59.254890: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-21 17:35:00.215439: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set the device",
   "id": "f7030101f006425f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:35:00.986747Z",
     "start_time": "2025-11-21T17:35:00.984336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "c6364fc6f794f7b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Importing the dataset",
   "id": "89f058ab04cd8ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:35:03.070773Z",
     "start_time": "2025-11-21T17:35:01.030666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get training and test set\n",
    "X_train, X_test, y_train, y_test = gts.get_data()\n",
    "X_train, X_test, y_train, y_test = np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train).unsqueeze(1)  # Add dimension for binary classification\n",
    "y_test = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n",
    "# Convert to CHW (Channels, Height, Width) from HWC (Height, Width, Channels)\n",
    "X_train = X_train.permute(0, 3, 1, 2)\n",
    "X_test = X_test.permute(0, 3, 1, 2)\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "if X_train.max() > 1.0:\n",
    "    X_train = X_train / 255.0\n",
    "    X_test = X_test / 255.0\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "2da2c67f381336b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Building the model",
   "id": "767e0e8a0849bc2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:35:03.910421Z",
     "start_time": "2025-11-21T17:35:03.127908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Building the model with Vision Transformer\n",
    "class ViTForBinaryClassification(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='google/vit-base-patch16-224'):\n",
    "        super(ViTForBinaryClassification, self).__init__()\n",
    "\n",
    "        # Load pretrained ViT\n",
    "        self.vit = ViTModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        # Get hidden size from ViT config\n",
    "        hidden_size = self.vit.config.hidden_size\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # Get ViT outputs\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "\n",
    "        # Use the [CLS] token representation (first token)\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        # Pass through classifier\n",
    "        logits = self.classifier(cls_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = ViTForBinaryClassification()\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\nModel created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ],
   "id": "ebaa11e43096baca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model created with 86401569 parameters\n",
      "Trainable parameters: 86401569\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the top layers of the model",
   "id": "2f5017bd98dab7e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:45:47.257110Z",
     "start_time": "2025-11-21T17:35:03.959172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Freeze all ViT layers\n",
    "for param in model.vit.parameters():\n",
    "    param.trainable = False\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\n=== Training Phase 1: Fine-tuning classifier only ===\")\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Train for 20 epochs\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/5 - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")"
   ],
   "id": "435530e5cd1adcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Phase 1: Fine-tuning classifier only ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [02:03<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.2576, Train Acc: 90.54% - Val Loss: 0.0512, Val Acc: 98.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [02:02<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 0.1079, Train Acc: 96.43% - Val Loss: 0.0271, Val Acc: 99.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [02:02<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train Loss: 0.0837, Train Acc: 97.12% - Val Loss: 0.0196, Val Acc: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [02:02<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train Loss: 0.0549, Train Acc: 98.43% - Val Loss: 0.0147, Val Acc: 99.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [02:02<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train Loss: 0.0433, Train Acc: 98.87% - Val Loss: 0.0116, Val Acc: 99.60%\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:03:39.569435Z",
     "start_time": "2025-11-21T17:03:39.567400Z"
    }
   },
   "cell_type": "markdown",
   "source": "### Visualise the layers",
   "id": "402cafe48794af80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:45:47.319780Z",
     "start_time": "2025-11-21T17:45:47.316975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Model Structure ===\")\n",
    "for name, module in model.named_children():\n",
    "    print(f\"{name}: {module.__class__.__name__}\")\n",
    "    if name == 'vit':\n",
    "        print(f\"  - ViT has {len(list(module.parameters()))} parameter tensors\")\n",
    "        print(f\"  - Encoder has {len(model.vit.encoder.layer)} transformer blocks\")"
   ],
   "id": "ebf7dfbb9a581a5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Structure ===\n",
      "vit: ViTModel\n",
      "  - ViT has 200 parameter tensors\n",
      "  - Encoder has 12 transformer blocks\n",
      "classifier: Sequential\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fine-tuning the transformer layers",
   "id": "3c3e41c8fd383ca2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:52:24.054063Z",
     "start_time": "2025-11-21T17:47:11.554045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Training Phase 2: Fine-tuning last transformer blocks ===\")\n",
    "\n",
    "# Unfreeze the ViT\n",
    "for param in model.vit.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Freeze embeddings and early encoder layers\n",
    "# Keep only the last 3 transformer blocks trainable\n",
    "num_blocks = len(model.vit.encoder.layer)\n",
    "blocks_to_train = 3\n",
    "\n",
    "for param in model.vit.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for i, block in enumerate(model.vit.encoder.layer):\n",
    "    if i < num_blocks - blocks_to_train:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "print(f\"Training last {blocks_to_train} transformer blocks out of {num_blocks}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# Use SGD with lower learning rate for fine-tuning\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                      lr=0.00001, momentum=0.9)\n",
    "\n",
    "# Fine-tune for 20 more epochs\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/5 - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")"
   ],
   "id": "f34f62f7a8a0ff83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Phase 2: Fine-tuning last transformer blocks ===\n",
      "Training last 3 transformer blocks out of 12\n",
      "Trainable parameters: 21868065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:57<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 0.0338, Train Acc: 99.25% - Val Loss: 0.0116, Val Acc: 99.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:56<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train Loss: 0.0337, Train Acc: 99.25% - Val Loss: 0.0116, Val Acc: 99.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:56<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train Loss: 0.0318, Train Acc: 99.19% - Val Loss: 0.0116, Val Acc: 99.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:56<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train Loss: 0.0362, Train Acc: 98.87% - Val Loss: 0.0116, Val Acc: 99.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:56<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train Loss: 0.0336, Train Acc: 99.31% - Val Loss: 0.0116, Val Acc: 99.60%\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Saving the model",
   "id": "927fcf3dd4bdbc22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:52:34.027192Z",
     "start_time": "2025-11-21T17:52:33.735478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the entire model\n",
    "torch.save(model.state_dict(), \"face_recognition_vit_pytorch_V1.pth\")\n",
    "print(\"\\nModel saved successfully!\")"
   ],
   "id": "e7ae82442865d23b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
