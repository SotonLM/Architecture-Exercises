For some of the prompts, the answers generated are facutal and concise, answering the question effectively.
But without the use of context, the answers generated are almost always unintelligble or extremely vague, making them very ineffective answers.
Upon observation, some of the outputs generated by the model when context was provided were just excerpts from the provided corpus, which meant that the model was not actually answering the question, instead just repeating what is alrady known.
Thankfully, all the prompts that had context provided answers that somewhat made sense, but a stable median or average answer from the outputs generated from the model, cannot be found with the current state of it.