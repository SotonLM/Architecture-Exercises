Level 2 findings:
generation time varied between 5-30 seconds

changing variable from max-length to max_new_tokens made some auto adjustments the terminal was making no longer necessary when set to higher numbers (100 and above)

increasing temperature a lot seems to make the model no longer able to concentrate on the prompt and go on ramblings, keep this relatively low

but low temperature seems to have repeating text? but when it doesn't repeat text it is making more sense

reducing top k means reducing output diversity

I have these massive gaps in my output, my file isn't broken it's just spread out due to this

the output I'll stick with has temp = 0.5, top_k = 50, max_new_tokens = 100
